{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC3160 Assignment 2\n",
    "Name: Xu Kai \n",
    "Student ID: 121090650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following codes are the answer of Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned content has been saved to output.txt\n",
      "The cleaned content of the html is : AIR6001 AI and Applications / MDS6105 Advanced AI AIR6001 AI and Applications MDS6105 Advanced AI Spring 2023 This course is more relevant than ever given the recent surge in Artificial Intelligence technologies. It introduces the fundamental concepts, history, and advanced technologies of AI in various applications, providing students with a comprehensive understanding of this rapidly evolving field. The course covers basic concepts of AI, such as intelligent agents, problem-solving, search, and first-order logic. It also delves into fundamental AI technologies, including regression, pattern recognition, sequential modeling, data mining, deep learning, and supervised modeling. Additionally, the course explores technologies used in various AI applications, such as anomaly detection, edge AI, image processing, speech processing, natural language processing (including machine translation and dialogue modeling), scientific paper analysis, AI in healthcare, autonomous driving, and DNN interpretability and bias. Teaching team Instructor Satoshi Nakamura TA Xi Chen Logistics Lectures: are on Tuesday/Thursday 1:30PM - 3:00PM in TXB201(TBD). Note: lectures will be remote for the first two weeks, and hybrid afterwards. The Zoom link is posted on BB. Office hours Prof. Nakamura: Thu 2:30-3:30 PM. Teaching Complex 704 TA. Xi Chen: Wed 7-9PM. Teaching Complex 703, Seat No.7 Contact: If you have any question, please reach out to us via email or post it to BB. Slack. Anyone is welcome to join the slack channel for discussion. --> Course Information This course is designed for students interested in artificial intelligence. The first half mainly covers the fundamentals of artificial intelligence, and the second half focuses on the applications of artificial intelligence in various fields. In particular, the topics include: AI History Intelligent Agents Deep Learning Image Processing Natural Language Processing Speech Processing Diallogue AI applications in Health and Self-driving Car, etc. Prerequisites Proficiency in LaTex: All the reports need to be written by using LaTex. A template will be provided. If you are not familiar with LaTex, please learn from the <a href=\"https://www.overleaf.com/learn/latex/Tutorials\" target=\"_blank\">tutorial in advance. Proficiency in GitHub: All the source codes need to be submitted in GitHub. Proficiency in Python: All the assignments will be in Python (using Numpy and <a href=\"https://pytorch.org/\" target=\"_blank\">PyTorch). Basic machine learning knowledge: It is possible to take this course without any machine learning knowledge, however, the course will be easier if you have foundations of machine learning. Basic Concepts of probability: It will be easier for you to understand some lectures if you know basics of probability. Textbooks Recommended Books: Artificial Intelligence: A Modern Approach 4th Edition , by Stuart Russell and Peter Norvig Grading Policy (AIR6001/MDS6105) Assignments (30%) Assignment 1 (10%): Lecture 1 - Lecture 6 Assignment 2 (10%): Lecture 7 - Lecture 11 Assignment 3 (10%): Lecture 12 - Lecture 26 Midterm exam (30%) (TBD) We will have a mid-term exam after lecture 14. The scope of the mid-term exam is from lecture 1 to lecture 14. Final exam (35%) (TBD) We will have a final exam on TBD. The scope of the final exam is from lecture 1 to lecture 28. Participation (5%) (TBD)Here are some ways to earn the participation credit, which is capped at 5%. Attending Guest lectures: In the second half of the course, we have four invited speakers. We encourage students to attend the guest lectures and participate in Q&A. All students get 0.75% per guest lecture (in total 3%) for either attending in person, or by writing a guest lecture report if they attend remotely or watch the recording. --> Completing feedback surveys: We will send out two feedback surveys during the semester to improve the course. All students get 0.5% per survey (in total 1%) for completing the surveys. --> Course and Teaching Evaluation (CTE): The school will send requests for CTE to all students. The CTE is worth 1% credit. Slack participation: The top 10 contributors (10 from CSC3100 and 10 from MDS6002) to slack will get 1%; others will get credit in propotion to the 10th contributor. --> Volunteer credit (1%): TAs/instuctor can nominate students for a volunteer credit for those who help the poster session organization, or help answer questions from other students (not writing assignments). --> Late Policy The penalty is 0.5% off the final course grade for each late day. Schedule Date Lecture Description Readings Lecture Note Events/Deadlines Sept 3 Lecture 1: Course Introduction/AI Introduction Learn LaTeX in 30 minutes Colab official tutorial Official tutorials of GitHub --> [Slides] Sept 5 Lecture 2: AI history/Inteligent agent [Slides] Sept 10 Lecture 3: AI Solving Problems by Searching Deep Learning in a Nutshell: Core Concepts <a href=\"https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained\">Machine learning, explained --> Slides] --> [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] --> Sept 12 Lecture 4: AI Informed Search Sept 19 Lecture 5: AI in Constraint satisfaction Problems Sept 24 Lecture 6: AI in Logical Agent and 1st order logic Assignment 1 out Sept 26 Lecture 7: AI Regression Oct 8 Lecture 8: AI Pattern Recognition Oct 10 Lecture 9: AI Sequence modeling Oct 15 Lecture 10: AI in Data mining Assignment 2 out Oct 17 Lecture 11: AI in Deep Learning Fundamentals 1 Oct 22 Lecture 12: AI in Deep Learning Fundamentals 2 Oct 24 Lecture 13: AI Self-supervised modeling Oct 29 Midterm exam Oct 31 Lecture 14: AI in Anormaly 1 Assignment 3 out Nov 5 Lecture 15: AI in Anormaly 2 Nov 7 Lecture 16: Edge AI Nov 12 Lecture 17: AI Image Processing Nov 14 Lecture 18: AI in Speech Processing Nov 19 Lecture 19: AI in Natural Language Processing Nov 21 Lecture 20: AI in Machine Translation 1 Nov 26 Lecture 21: AI in Machine Translation 2 Nov 28 Lecture 22: AI in Dialogue System Dec 3 no class (SLT2024) Dec 5 Lecture 23: AI in Health, Medical Image Processing I Dec 10 Lecture 24: AI in Self-driving Car Dec 12 Lecture 25: AI in Interpretability and Ethics Jan 11 Tutorial 1: PyTorch PyTorch QuickstartsPyTorch Installation [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:v:/g/personal/222042021_link_cuhk_edu_cn/EWB7I5yqp9NLtkgaHsWFSJMB7USvic0K8HW4NGIMp7Qllg?e=eBZ4Ko\">Video] [<a href=\"https://colab.research.google.com/drive/1_dpTtjt91Gw1u80K5hPQp8SdH0ir_OjA?usp=sharing\">Colab] --> Jan 12 Lecture 4: Solving problems by searching Pitch, loudness and timbre<a href=\"https://newt.phys.unsw.edu.au/jw/sound.spectrum.html\">What is a Sound Spectrum? [Slides] [<a href=\"./materials/lecture_notes/Lecture_3_sound/index.html\">HTML] [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] [<a href=\"https://github.com/SLPcourse/SLPcourse.github.io/blob/main/materials/lecture_notes/code\">Code] Assignment 1 out Jan 15 Tutorial 2: TorchAudio (by Torchaudio team) TorchAudio Documentation [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] 10:00am via zoom Jan 17 Lecture 4: Understanding human speech Voice Acoustics: an introduction<a href=\"https://speechprocessingbook.aalto.fi/index.html\">Introduction to Speech Processing [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] [<a href=\"https://github.com/SLPcourse/SLPcourse.github.io/blob/main/materials/lecture_notes/code\">Code] Feb 9 Lecture 5: Human sounds and their organization Chapter 25: Phonetics [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] Feb 14 Lecture 6: Text processing and regular expressions Chapter 2: Regular Expressions, Text Normalization, Edit Distance [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] Assignment 2 outAssignment 1 due (11:59pm) Feb 15 Tutorial 3: Text processing Python Regular Expression DocumentationNLTK Tokenize Documentation [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:v:/g/personal/222043002_link_cuhk_edu_cn/EV4yKn5BALtJhEHWTUj01bsBI5uwMVH97OZkmWvpxuP9xw?e=zGpBOM\">Video][<a href=\"https://colab.research.google.com/drive/1IKPG0tLZwvmM8lNVK3ddZ3yYp-35Ky1Y?usp=sharing\">Colab] Feb 15 Project Release Singing Voice Conversion Project Detecting Generated Abstract Project Voice Spoofing Detection Project [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:v:/g/personal/222042021_link_cuhk_edu_cn/EQZ5H7mFY-9CmpfBRkH41w4Bi4DakZ2-IFqbu9TQAcJcPw?e=jE5sN1\">Video] Feb 16 Lecture 7: Words and their relationship to other words Chapter 8: Sequence Labeling for Parts of Speech and Named Entities [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] Feb 21 Lecture 8: Syntax: Structure of sentences [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] Feb 23 Lecture 9: Language models Chapter 3: N-gram Language ModelsChapter 7: Neural Networks and Neural Language Models [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] Assignment 2 due (11:59pm) Feb 28 Lecture 10: Language models Chapter 3: N-gram Language ModelsChapter 7: Neural Networks and Neural Language Models<a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] Mar 2 Lecture 11: Embedding: Representations of the meaning of words Chapter 6: Vector Semantics and Embeddings [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] Project proposal due (11:59pm) Mar 7 Lecture 12: Embedding: Representations of the meaning of words Chapter 6: Vector Semantics and Embeddings [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] Mar 8 Tutorial 4: Word embedding [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:v:/g/personal/222043002_link_cuhk_edu_cn/Ee-9mcHyv0dMqoloc6yf668B65ty_P_2gzEmipqf5PebHg?e=vs1JaV\">Video] [<a href=\"https://colab.research.google.com/drive/1TRQyU5MtWn9DTuFCnKjbI1cjxh2mh_KW?usp=sharing\">Colab] Mar 9 Midterm exam Assignment 3 out Mar 14 Word embedding [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] Mar 15 Tutorial 5: Visualization and plotting Case Study - Zoom Out and Observe: News Environment Perception for Fake News Detection [Slides] [<a href=\"https://colab.research.google.com/drive/1sKAiDmvymnM4np5As3fcnPUtsPt7oaqn?usp=sharing\">Colab] [<a href=\"https://cuhko365-my.sharepoint.com/:v:/g/personal/222042021_link_cuhk_edu_cn/EUCrBwhBexlNq73LOhIO89wBRAdsu2jkupn3O8KzkI5vvQ?e=CBodpg\">Video] Mar 16 Lecture 13: SLP Application - Sentiment analysis [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] Mar 21 Lecture 14: SLP Application - Text summarization [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] Assignment 3 due (11:59pm) Mar 22 Lecture 15: Summarizing Conversations: From Meetings to Social Media (by <a href=\"https://sites.google.com/site/nancyfchen/home\">Nancy Chen) Invited talk. Location: DY103, Time: 12-13 Mar 28 Lecture 16: SLP Application - Fundamentals of speech recognition (by <a href=\"https://www.linkedin.com/in/xiong-xiao-0a6a9b29/\">Xiong Xiao) [Slides] Invited guest lecture Mar 30 Mid-term break Project milestone 1 due (11:59pm) Apr 6 Lecture 17: SLP Application - Voice conversion(by Shuai Wang) [Slides] Invited guest lecture Apr 11 Final project development In-class office hour Apr 13 Final project development In-class office hour Apr 18 Lecture 18: SLP Application - Text-to-speech synthesis [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] Project milestone 2 due (11:59pm) Apr 20 Lecture 19: SLP Application - Machine translation Chapter 13: Machine Translation [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] Apr 25 Lecture 20: SLP Application - Question answering Chapter 14: Question Answering [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] Apr 27 Lecture 21: SLP Application - Chatbot Chapter 15: Chatbots and Dialogue Systems [Slides] [<a href=\"https://cuhko365-my.sharepoint.com/:f:/g/personal/wuzhizheng_cuhk_edu_cn/EkWiGYtWog5KiDVKJzwUo6YBG80UgOEkmvZk27aPbzYx5g?e=ciFnnG\">Video] May 4 No class How to write the final report? Final project report early submission due (11:59pm) May 9 No class May 11 Final project report due (11:59pm) May 21 Final project poster session This session is open to the CUHK-Shenzhen community and invited guests. Details will be available soon. Time: 9am - 12:00pm for poster session, 1:30pm - 5:30pm for talks from external experts. There will be companies offering full-time job and internship opportunities. -->\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_html(html_content):\n",
    "    # Use regular expressions to remove HTML tags\n",
    "    clean_text = re.sub(r'<.*?>', '', html_content)\n",
    "    # Remove extra whitespace characters\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "    return clean_text\n",
    "\n",
    "# File paths\n",
    "input_file_path = 'input.html'  # Replace with the path to your HTML file\n",
    "output_file_path = 'output.txt'  # Replace with the desired path for the TXT file\n",
    "\n",
    "# Read HTML file content\n",
    "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Clean the HTML content\n",
    "cleaned_text = clean_html(html_content)\n",
    "\n",
    "# Write the cleaned content to a TXT file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(cleaned_text)\n",
    "\n",
    "print(f'Cleaned content has been saved to {output_file_path}')\n",
    "print(f'The cleaned content of the html is : {cleaned_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following codes are the answer of Task 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Vocabulary: {'b', 'a', 'd', 'c', ' '}\n",
      "-------------------------------------------------------\n",
      "Vocabulary after 1th iteration: {'aa', 'b', 'a', 'd', 'c', ' '}\n",
      "The new token at 1th iteration is \"aa\"\n",
      "-------------------------------------------------------\n",
      "Vocabulary after 2th iteration: {'ab', 'aa', 'b', 'a', 'd', 'c', ' '}\n",
      "The new token at 2th iteration is \"ab\"\n",
      "-------------------------------------------------------\n",
      "Tokenized Output: {aa}{ab}d{aa}{ab}{ab}c {aa}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Function to find the most frequent pair of adjacent tokens\n",
    "def get_most_frequent_pair(corpus):\n",
    "    # Create a list of all pairs of adjacent characters\n",
    "    pairs = Counter()\n",
    "    for i in range(len(corpus) - 1):\n",
    "        pairs[(corpus[i], corpus[i + 1])] += 1\n",
    "    return pairs.most_common(1)[0] if pairs else None\n",
    "\n",
    "\n",
    "def replace_substring(original_string, N, M):\n",
    "    # Replace all occurrences of N with M\n",
    "    modified_string = original_string.replace(N, M)\n",
    "    return modified_string\n",
    "\n",
    "\n",
    "def BPE(stringC, numMergeK):\n",
    "   \n",
    "    # Initialize the vocabulary with unique characters\n",
    "    vocab = set(stringC)\n",
    "    vocab_init = vocab\n",
    "    corpus = stringC\n",
    "    print(f'Initial Vocabulary: {vocab}')\n",
    "    print('-------------------------------------------------------')\n",
    "\n",
    "    # Track the tokens with their frequencies\n",
    "    index_list = [chr(200 + i) for i in range(numMergeK)]\n",
    "    or_list  = []\n",
    "    new_append = []\n",
    "\n",
    "    for i in range(numMergeK):\n",
    "        # Get the most frequent pair of adjacent tokens\n",
    "        most_frequent_pair = get_most_frequent_pair(corpus)\n",
    "        if not most_frequent_pair:\n",
    "            break\n",
    "\n",
    "        tL, tR = most_frequent_pair[0]\n",
    "        tNEW = tL + tR\n",
    "\n",
    "        # Update vocabulary\n",
    "        vocab.add(tNEW)\n",
    "        or_list.append('{'+ tNEW + '}')\n",
    "        new_append.append(tNEW)\n",
    "        print(f'Vocabulary after {i+1}th iteration: {vocab}')\n",
    "        print(f'The new token at {i+1}th iteration is \"{tNEW}\"')\n",
    "        print('-------------------------------------------------------')\n",
    "\n",
    "        # Replace occurrences of tL and tR in the corpus\n",
    "        corpus = replace_substring(corpus,tNEW,index_list[i])\n",
    "\n",
    "        # Print the current state\n",
    "        #print(f\"Iteration {i + 1}: Replaced {tL} and {tR} with {tNEW}\")\n",
    "        #print(f\"Current corpus: {corpus}\")\n",
    "    \n",
    "    # Final tokenization\n",
    "    for j in range(numMergeK):\n",
    "        corpus = replace_substring(corpus,index_list[j],or_list[j])\n",
    "\n",
    "    tokenized_output = corpus\n",
    "\n",
    "\n",
    "    return tokenized_output, new_append\n",
    "\n",
    "# Input text and number of merges\n",
    "text = 'aaabdaaababc aa'\n",
    "k = 2\n",
    "\n",
    "# Run Byte-Pair Encoding\n",
    "output,_= BPE(text, k)\n",
    "print(f\"Tokenized Output: {output}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following codes are the answer of Task 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 1th iteration, the most frequent pair is ['e', ' '] with occurance 222,the new token is \"e \"\n",
      "At 2th iteration, the most frequent pair is ['i', 'n'] with occurance 204,the new token is \"in\"\n",
      "At 3th iteration, the most frequent pair is ['o', 'n'] with occurance 126,the new token is \"on\"\n",
      "At 4th iteration, the most frequent pair is ['s', ' '] with occurance 116,the new token is \"s \"\n",
      "At 5th iteration, the most frequent pair is ['r', 'e'] with occurance 115,the new token is \"re\"\n",
      "At 6th iteration, the most frequent pair is ['e', 'c'] with occurance 111,the new token is \"ec\"\n",
      "At 7th iteration, the most frequent pair is ['e', 'n'] with occurance 102,the new token is \"en\"\n",
      "At 8th iteration, the most frequent pair is ['a', 'l'] with occurance 101,the new token is \"al\"\n",
      "At 9th iteration, the most frequent pair is ['u', 'r'] with occurance 100,the new token is \"ur\"\n",
      "At 10th iteration, the most frequent pair is ['e', 'r'] with occurance 100,the new token is \"er\"\n",
      "At 11th iteration, the most frequent pair is ['t', ' '] with occurance 98,the new token is \"t \"\n",
      "At 12th iteration, the most frequent pair is [':', ' '] with occurance 92,the new token is \": \"\n",
      "At 13th iteration, the most frequent pair is ['a', 'n'] with occurance 87,the new token is \"an\"\n",
      "At 14th iteration, the most frequent pair is ['in', 'g'] with occurance 85,the new token is \"ing\"\n",
      "At 15th iteration, the most frequent pair is ['t', 'i'] with occurance 83,the new token is \"ti\"\n",
      "At 16th iteration, the most frequent pair is ['d', 'e'] with occurance 81,the new token is \"de\"\n",
      "At 17th iteration, the most frequent pair is ['o', 'r'] with occurance 75,the new token is \"or\"\n",
      "At 18th iteration, the most frequent pair is ['ec', 't'] with occurance 75,the new token is \"ect\"\n",
      "At 19th iteration, the most frequent pair is ['d', ' '] with occurance 68,the new token is \"d \"\n",
      "At 20th iteration, the most frequent pair is ['a', 'r'] with occurance 68,the new token is \"ar\"\n"
     ]
    }
   ],
   "source": [
    "def get_most_frequent_pair_1(corpus):\n",
    "    # Create a list of all pairs of adjacent characters\n",
    "    pairs = Counter()\n",
    "    for i in range(len(corpus) - 1):\n",
    "        pairs[(corpus[i], corpus[i + 1])] += 1\n",
    "        \n",
    "    # Get the most common pair and its count\n",
    "    most_common_pair = pairs.most_common(1)\n",
    "    return most_common_pair[0] if most_common_pair else None\n",
    "\n",
    "\n",
    "def BPE_1(stringC, numMergeK):\n",
    "   \n",
    "    # Initialize the vocabulary with unique characters\n",
    "    vocab = []\n",
    "    corpus = stringC\n",
    "\n",
    "    # Track the tokens with their frequencies\n",
    "    index_list = [chr(200 + i) for i in range(numMergeK)]\n",
    "\n",
    "    times = []\n",
    "    pairs = []\n",
    "\n",
    "    for i in range(numMergeK):\n",
    "        # Get the most frequent pair of adjacent tokens\n",
    "        most_frequent_pair = get_most_frequent_pair_1(corpus)\n",
    "        if not most_frequent_pair:\n",
    "            break\n",
    "\n",
    "        tL, tR = most_frequent_pair[0][0],most_frequent_pair[0][1]\n",
    "        pairs.append([tL,tR])\n",
    "        occurrence = most_frequent_pair[1]\n",
    "        times.append(occurrence)\n",
    "        tNEW = tL + tR\n",
    "        \n",
    "        vocab.append(tNEW)\n",
    "\n",
    "        # Replace occurrences of tL and tR in the corpus\n",
    "        corpus = replace_substring(corpus,tNEW,index_list[i])\n",
    "                \n",
    "    return index_list,vocab,times,pairs\n",
    "\n",
    "\n",
    "# Input text and number of merges\n",
    "text = cleaned_text\n",
    "k = 20\n",
    "\n",
    "# Run Byte-Pair Encoding\n",
    "A,B,C,D = BPE_1(text, k)\n",
    "\n",
    "#old_B = B.copy()\n",
    "\n",
    "for i in range(20):\n",
    "    item = list(B[i])\n",
    "    for j in range(len(item)):\n",
    "        if item[j] in A:\n",
    "            ind = A.index(item[j])\n",
    "            item[j] = B[ind]\n",
    "    B[i] = item     \n",
    "\n",
    "new_B = []\n",
    "for i in range(len(B)):\n",
    "    sublist = B[i]\n",
    "    ele = ''\n",
    "    for j in range(len(sublist)):\n",
    "        if type(sublist[j]) == str: \n",
    "            ele = ele + sublist[j]\n",
    "        else:\n",
    "            ele_1 = ''\n",
    "            for k in range(len(sublist[j])):\n",
    "                ele_1 = ele_1 + sublist[j][k]\n",
    "            ele = ele +ele_1\n",
    "    new_B.append(ele)\n",
    "\n",
    "\n",
    "for i in range(len(D)):\n",
    "    for j in range(2):\n",
    "        if D[i][j] in A:\n",
    "            ind = A.index(D[i][j])\n",
    "            D[i][j] = new_B[ind]\n",
    "\n",
    "for i in range(20):\n",
    "    print(f'At {i+1}th iteration, the most frequent pair is {D[i]} with occurance {C[i]},the new token is \"{new_B[i]}\"')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following codes are the answer of Task 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three longest tokens is:\n",
      "'ing', with length: 3\n",
      "'ect', with length: 3\n",
      "'e ', with length: 2\n"
     ]
    }
   ],
   "source": [
    "top_three_longest = sorted(new_B, key=len, reverse=True)[:3]\n",
    "\n",
    "print(\"The three longest tokens is:\")\n",
    "for s in top_three_longest:\n",
    "    print(f\"'{s}', with length: {len(s)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion for 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For 'ing': This token represents a common English suffix that indicates ongoing action or a gerund form of verbs. Its length (3 characters) makes it one of the longest tokens, and its frequent usage in the language enhances its importance. <p>\n",
    "- 'ect': This token often appears in words related to science or specific fields (e.g., \"select,\" \"detect\"). Its length also matches 'ing', making it a significant subword for creating new terms in various contexts. <p>\n",
    "- 'e ': Although this token is short, it includes a space, indicating its use in word formation. It can signify a common pattern in certain linguistic contexts, making it noteworthy despite its length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following codes are the answer of Task 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ahref=\"https://\n",
      "-------------\n",
      "<ahref=\"https://\n",
      "-------------\n",
      "<ahref=\"https:/\n",
      "-------------\n",
      "<ahref=\"https\n",
      "-------------\n",
      "<ahref=\"http\n",
      "-------------\n",
      "<ahref=\"htt\n",
      "-------------\n",
      "<ahref=\"h\n",
      "-------------\n",
      "<ahref=\"\n",
      "-------------\n",
      "Lecture\n",
      "-------------\n",
      "ecture\n",
      "-------------\n",
      "<ahref\n",
      "-------------\n",
      "Slides\n",
      "-------------\n",
      "Slides\n",
      "-------------\n",
      "Slides\n",
      "-------------\n",
      "person\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "def convert(A,B,D,K):\n",
    "    for i in range(K):\n",
    "        item = list(B[i])\n",
    "        for j in range(len(item)):\n",
    "            if item[j] in A:\n",
    "                ind = A.index(item[j])\n",
    "                item[j] = B[ind]\n",
    "        B[i] = item     \n",
    "\n",
    "    new_B = [''.join(map(str, item)) for item in B]\n",
    "\n",
    "\n",
    "    for i in range(len(D)):\n",
    "        for j in range(2):\n",
    "            if D[i][j] in A:\n",
    "                ind = A.index(D[i][j])\n",
    "                D[i][j] = new_B[ind]\n",
    "    return new_B, D\n",
    "\n",
    "K = 100\n",
    "A,B,C,D = BPE_1(text, K)\n",
    "B, D = convert(A,B,D,K=100)\n",
    "B = [s.replace('[', '').replace(']', '').replace(',', '').replace(\"'\", '').replace(' ','') for s in B]\n",
    "\n",
    "sortest_longest = sorted(B, key=len, reverse=True)\n",
    "for i in range(15):\n",
    "    print(sortest_longest[i])\n",
    "    print('-------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three longest tokens are:\n",
      "<ahref=\"https://\n",
      "Lecture\n",
      "Slides\n"
     ]
    }
   ],
   "source": [
    "print(\"The three longest tokens are:\")\n",
    "print('<ahref=\"https://')\n",
    "print('Lecture')\n",
    "print('Slides')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
