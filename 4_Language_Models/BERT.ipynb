{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "## References\n",
    "- Paper 1: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
    "- CSDN 1: [一文读懂BERT(原理篇)](https://blog.csdn.net/jiaowoshouzi/article/details/89073944)\n",
    "- CSDN 2 : [一文彻底搞懂 Bert（图解+代手撕)](https://blog.csdn.net/weixin_42029738/article/details/139578563)\n",
    "- [知乎](https://zhuanlan.zhihu.com/p/403495863)\n",
    "## 1.Introduction \n",
    "- **Def** : BERT is a method of **pre-training** language representations, meaning that we train a general-purpose \"language understanding\" <br> model on a large text corpus (like Wikipedia), and then use that model for downstream NLP tasks that we care about (like question answering). <br> BERT outperforms previous methods because it is the first **unsupervised**, deeply **bidirectional** system for pre-training NLP. <p>\n",
    "- **Usage** : BERT 用于高效地将高度非结构化的文本数据表示为向量。<p>\n",
    "- **Structure** : BERT是一个经过训练的 Transformer 编码器堆栈。BERT 官网最开始提供了两个版本，如下图所示。这里L表示的是transformer encoders的层数，H表示输出的维度，A表示mutil-head attention的个数\n",
    "\n",
    "<figure>\n",
    "<img src= \"https://i-blog.csdnimg.cn/blog_migrate/87ec2626f972612291b78fb680a3f2fa.png\" width=600/>\n",
    "</figure>\n",
    "\n",
    "- **Why we need BERT** : 当我们已经有词嵌入时，为什么我们还需要 BERT? 因为一个词在不同的上下文中可能有不同的含义。例如，`I encountered a bat when I went to buy a cricket bat`, 这里，第一次出现的`bat`，指的是一种哺乳动物，第二次出现的指的是一只球拍。在这种情况下，`bat`这个词的第一次和第二次出现需要以不同的方式表示，因为它们的含义不同，但是词嵌入将它视为相同的词。因此，将生成单个词`bat`的表示。这将导致错误的预测。BERT 嵌入将能够通过为同一个词`bat`生成两个不同的向量来区分和捕捉两个不同的语义含义。\n",
    "\n",
    "- **Why we call 'Bidirectional'** : BERT是用了Transformer的encoder侧的网络，encoder中的Self-attention机制在编码一个token的时候同时利用了其上下文的token，其中‘同时利用上下文’即为双向的体现.\n",
    "\n",
    "- **Overview of BERT**\n",
    "\n",
    "<figure>\n",
    "<img src= \"https://pica.zhimg.com/v2-358f44e95ed4e734fbb84013497c5de8_1440w.jpg\" width=600/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details\n",
    "\n",
    "### 2.1 Embedding\n",
    "\n",
    "BERT 中的 Embedding由三种Embedding求和而成：\n",
    "\n",
    "\n",
    "Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任务."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
