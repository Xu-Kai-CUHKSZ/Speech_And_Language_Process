{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "## References\n",
    "- Paper 1: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
    "- CSDN 1: [一文读懂BERT(原理篇)](https://blog.csdn.net/jiaowoshouzi/article/details/89073944)\n",
    "- CSDN 2 : [一文彻底搞懂 Bert（图解+代手撕)](https://blog.csdn.net/weixin_42029738/article/details/139578563)\n",
    "- [知乎](https://zhuanlan.zhihu.com/p/403495863)\n",
    "\n",
    "\n",
    "\n",
    "## 1.Introduction of BERT\n",
    "- **Def** : BERT is a method of **pre-training** language representations, meaning that we train a general-purpose \"language understanding\" <br> model on a large text corpus (like Wikipedia), and then use that model for downstream NLP tasks that we care about (like question answering). <br> BERT outperforms previous methods because it is the first **unsupervised**, deeply **bidirectional** system for pre-training NLP. <p>\n",
    "- **Usage** : BERT 用于高效地将高度非结构化的文本数据表示为向量。<p>\n",
    "- **Structure** : BERT是一个经过训练的 Transformer 编码器堆栈。BERT 官网最开始提供了两个版本，如下图所示。这里L表示的是transformer encoders的层数，H表示输出的维度，A表示mutil-head attention的个数\n",
    "\n",
    "<figure>\n",
    "<img src= \"https://i-blog.csdnimg.cn/blog_migrate/87ec2626f972612291b78fb680a3f2fa.png\" width=600/>\n",
    "</figure>\n",
    "\n",
    "- **Why we need BERT** : 当我们已经有词嵌入时，为什么我们还需要 BERT? 因为一个词在不同的上下文中可能有不同的含义。例如，`I encountered a bat when I went to buy a cricket bat`, 这里，第一次出现的`bat`，指的是一种哺乳动物，第二次出现的指的是一只球拍。在这种情况下，`bat`这个词的第一次和第二次出现需要以不同的方式表示，因为它们的含义不同，但是词嵌入将它视为相同的词。因此，将生成单个词`bat`的表示。这将导致错误的预测。BERT 嵌入将能够通过为同一个词`bat`生成两个不同的向量来区分和捕捉两个不同的语义含义。\n",
    "\n",
    "- **Why we call 'Bidirectional'** : BERT是用了Transformer的encoder侧的网络，encoder中的Self-attention机制在编码一个token的时候同时利用了其上下文的token，其中‘同时利用上下文’即为双向的体现.\n",
    "\n",
    "- **Overview of BERT**\n",
    "\n",
    "<figure>\n",
    "<img src= \"https://upload.wikimedia.org/wikipedia/commons/b/b5/BERT_embeddings_01.png\" width=800/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture of BERT\n",
    "\n",
    "### 2.1 Embedding\n",
    "\n",
    "BERT 中的 Embedding由三种Embedding求和而成：\n",
    "\n",
    "<figure>\n",
    "<img src= \"https://upload.wikimedia.org/wikipedia/commons/6/65/BERT_input_embeddings.png\" width=800/>\n",
    "</figure>\n",
    "\n",
    "- `Token Embeddings`是词向量，第一个单词是`CLS`标志，可以用于之后的分类任务. 特别的，英文词汇会做更细粒度的切分，比如`playing` 或切割成 `play `和 `##ing`，中文目前尚未对输入文本进行分词，直接对单子构成为本的输入单位。将词切割成更细粒度的 Word Piece 是为了解决未登录词的常见方法。<p>\n",
    "- `Segment Embeddings`用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务. <p>\n",
    "- `Position Embeddings`和之前文章中的Transformer不一样，不是三角函数而是学习出来的.<p>\n",
    "- **[CLS]的作用**:BERT在第一句前会加一个[CLS]标志，最后一层该位对应向量可以作为整句话的语义表示，从而用于下游的分类任务等。因为与文本中已有的其它词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个词的语义信息，从而更好的表示整句话的语义。 具体来说，self-attention是用文本中的其它词来增强目标词的语义表示，但是目标词本身的语义还是会占主要部分的，因此，经过BERT的12层（BERT-base为例），每次词的embedding融合了所有词的信息，可以去更好的表示自己的语义。而[CLS]位本身没有语义，经过12层，句子级别的向量，相比其他正常词，可以更好的表征句子语义。\n",
    "\n",
    "###  2.2 Transformer Encoder\n",
    "- BERT是用了Transformer的encoder侧的网络.在Transformer中，模型的输入会被转换成512维的向量，然后分为8个head，每个head的维度是64维，但是BERT的维度是768维度，然后分成12个head，每个head的维度是64维，这是一个微小的差别。Transformer中position Embedding是用的三角函数，BERT中也有一个Postion Embedding是随机初始化，然后从数据中学出来的。\n",
    "\n",
    "\n",
    "## 3. Pre-training\n",
    "- **Concept of pre-trainign**:  `BERT` basically uses the concept of pre-training the model on a very large dataset in an unsupervised manner for language modeling. A pre-trained model on a very large dataset has the capability to better understand the context of the input sentence. After pre-training, the model can be fine-tuned on the task-specific supervised dataset to achieve good results.\n",
    "-\n",
    "### 3.1 MLM (Masked language models)\n",
    "- **Intuition** : Information from the future can be helpful for language understanding\n",
    "### 3.2 NSP\n",
    "\n",
    "\n",
    "## 4. Fine-tuning\n",
    "\n",
    "\n",
    "## 5. How to use BERT\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
