{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers with Its Application in Machine Translation\n",
    "\n",
    "- Paper: [Attention All You Need](https://arxiv.org/abs/1706.03762), [Layer Normalization](https://arxiv.org/abs/1607.06450)\n",
    "- Other References: [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/), [Attention 和 Self-Attention](https://blog.csdn.net/weixin_68191319/article/details/129218551?spm=1001.2014.3001.5502),  [万字长文,小白都能看得懂的 Transformer 解析(图解版)](https://zhuanlan.zhihu.com/p/660267333)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Attention and Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformer\n",
    "### 2.1 Introduction\n",
    "-  首先将`Transformer`这个模型看成是一个黑箱操作。在机器翻译中，就是输入一种语言，输出另一种语言，如下左图。拆开这个黑箱，我们可以看到它是由编码组件、解码组件和它们之间的连接组成，如下右图。\n",
    "<figure>\n",
    "<img src=\"https://jalammar.github.io/images/t/the_transformer_3.png\" width=600/>\n",
    "<img src=\"https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png\" width=400/>\n",
    "</figure>\n",
    "\n",
    "- 编码组件部分由一堆编码器（encoder）构成（论文中是将6个编码器叠在一起——数字6没有什么神奇之处，你也可以尝试其他数字）。<p> 解码组件部分也是由相同数量（与编码器对应）的解码器（decoder）组成的。从下图我们可以看出 Encoders 的输出会和每一层 Decoder 进行结合。原因是，Encoder 向每层的 Decoder 输入 KV ，Decoder 产生的Q 从Encoder KV 里 查询信息（下文会讲）。\n",
    "<figure>\n",
    "<img src=\"https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png\" width=800/>\n",
    "</figure>\n",
    "\n",
    "- 所有的编码器在结构上都是相同的，但它们没有共享参数。每个解码器都可以分解成两个子层。解码器中也有编码器的自注意力（self-attention）层<p>和前馈（feed-forward）层。除此之外，这两个层之间还有一个注意力层，用来关注输入句子的相关部分（和seq2seq模型的注意力作用相似）。\n",
    "<figure>\n",
    "<img src=\"https://jalammar.github.io/images/t/Transformer_encoder.png\" width=500/>\n",
    "<img src=\"https://jalammar.github.io/images/t/Transformer_decoder.png\" width=500/>\n",
    "</figure>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
